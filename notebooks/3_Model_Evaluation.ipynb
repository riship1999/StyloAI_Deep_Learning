{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Analysis ðŸ“Š\n",
    "\n",
    "This notebook covers:\n",
    "1. Model Performance Metrics\n",
    "2. A/B Testing Analysis\n",
    "3. Feature Importance\n",
    "4. Error Analysis\n",
    "5. Recommendation Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, ndcg_score\n",
    "import joblib\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load models\n",
    "cf_model = joblib.load('../models/cf_model.joblib')\n",
    "cb_model = joblib.load('../models/cb_model.joblib')\n",
    "hybrid_model = joblib.load('../models/hybrid_model.joblib')\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load('../data/X_test.npy')\n",
    "y_test = np.load('../data/y_test.npy')\n",
    "\n",
    "print(\"Models and data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'ndcg': ndcg_score([y_true], [y_pred])\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Get predictions from each model\n",
    "cf_predictions = cf_model.predict(X_test)\n",
    "cb_predictions = cb_model.predict(X_test)\n",
    "hybrid_predictions = hybrid_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "models = ['Collaborative Filtering', 'Content-Based', 'Hybrid']\n",
    "predictions = [cf_predictions, cb_predictions, hybrid_predictions]\n",
    "\n",
    "for model_name, preds in zip(models, predictions):\n",
    "    metrics = calculate_metrics(y_test, preds)\n",
    "    print(f\"\\n{model_name} Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A/B Testing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "\n",
    "def ab_test_analysis(control_metrics, test_metrics):\n",
    "    # Perform t-test\n",
    "    t_stat, p_value = stats.ttest_ind(control_metrics, test_metrics)\n",
    "    \n",
    "    # Calculate effect size\n",
    "    effect_size = (np.mean(test_metrics) - np.mean(control_metrics)) / np.std(control_metrics)\n",
    "    \n",
    "    return {\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': effect_size\n",
    "    }\n",
    "\n",
    "# Compare Hybrid vs CF model\n",
    "ab_results = ab_test_analysis(\n",
    "    cf_predictions,\n",
    "    hybrid_predictions\n",
    ")\n",
    "\n",
    "print(\"A/B Test Results (Hybrid vs CF):\")\n",
    "for metric, value in ab_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_feature_importance(model, feature_names):\n",
    "    # Get feature importance scores\n",
    "    importance_scores = model.feature_importances_\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance_scores\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance_df, x='importance', y='feature')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_names = ['Category', 'Brand', 'Price', 'Color', 'Season']\n",
    "importance_df = analyze_feature_importance(hybrid_model, feature_names)\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(importance_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_errors(y_true, y_pred, X_test):\n",
    "    # Calculate errors\n",
    "    errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Create error analysis DataFrame\n",
    "    error_df = pd.DataFrame({\n",
    "        'true': y_true,\n",
    "        'predicted': y_pred,\n",
    "        'error': errors,\n",
    "        'features': list(X_test)\n",
    "    })\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=error_df, x='error', bins=50)\n",
    "    plt.title('Error Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze high error cases\n",
    "    high_errors = error_df[error_df['error'] > error_df['error'].quantile(0.95)]\n",
    "    \n",
    "    return high_errors\n",
    "\n",
    "# Perform error analysis\n",
    "high_errors = analyze_errors(y_test, hybrid_predictions, X_test)\n",
    "print(\"\\nHigh Error Cases Analysis:\")\n",
    "print(high_errors.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendation Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def assess_recommendation_quality(model, test_cases):\n",
    "    quality_metrics = {\n",
    "        'relevance': [],\n",
    "        'diversity': [],\n",
    "        'novelty': []\n",
    "    }\n",
    "    \n",
    "    for case in test_cases:\n",
    "        # Get recommendations\n",
    "        recs = model.recommend(case)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        quality_metrics['relevance'].append(calculate_relevance(recs))\n",
    "        quality_metrics['diversity'].append(calculate_diversity(recs))\n",
    "        quality_metrics['novelty'].append(calculate_novelty(recs))\n",
    "    \n",
    "    # Plot quality metrics\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i, (metric, values) in enumerate(quality_metrics.items(), 1):\n",
    "        plt.subplot(1, 3, i)\n",
    "        sns.boxplot(y=values)\n",
    "        plt.title(f'{metric.capitalize()} Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# Assess recommendation quality\n",
    "test_cases = np.random.choice(len(X_test), 100)\n",
    "quality_metrics = assess_recommendation_quality(hybrid_model, test_cases)\n",
    "\n",
    "print(\"\\nRecommendation Quality Summary:\")\n",
    "for metric, values in quality_metrics.items():\n",
    "    print(f\"{metric}: {np.mean(values):.4f} Â± {np.std(values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compile all evaluation results\n",
    "evaluation_results = {\n",
    "    'performance_metrics': {\n",
    "        'cf': calculate_metrics(y_test, cf_predictions),\n",
    "        'cb': calculate_metrics(y_test, cb_predictions),\n",
    "        'hybrid': calculate_metrics(y_test, hybrid_predictions)\n",
    "    },\n",
    "    'ab_test_results': ab_results,\n",
    "    'feature_importance': importance_df.to_dict(),\n",
    "    'quality_metrics': quality_metrics\n",
    "}\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "with open('../results/evaluation_results.json', 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=4)\n",
    "\n",
    "print(\"Evaluation results saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
